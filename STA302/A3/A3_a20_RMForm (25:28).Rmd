---
title: "A Multiple Linear Model for Toronto and Mississauga House Prices"
author: "Elizabeth Riese, Id 1003106122"
date: "November 27, 2020"
output:
  pdf_document: default
  html_document:
    df_print: paged
---
## I. Data Wrangling

150 random samples from the real estate data set were generated and their respective ID numbers are reported below.

```{r, echo=F}
#I.a)

library(readr)
real203 <- read_csv("real203.csv") # getting Rstudio to read the file and load it

set.seed(1003106122) #setting seed to student number to work with the same sample drawn from the data
real_data <- as.data.frame(real203) 
# converting to data frame to make manipulation of the data easier later on
sample0 <- real_data[sample(nrow(real_data), 150), ] 
# sampling 150 random values from the real203 data set
sample0[,"ID"] # prints out all the ID numbers of the properties that were generated by the random selection
```

```{r, echo=F}
# I. b)
set.seed(1003106122)
lotsize = (sample0["lotwidth"])*(sample0["lotlength"]) # called the lotwidth and lotlength data from the subset then multiplied to get the lotsize.
sample0$lotlength <- NULL
sample0$lotwidth <- NULL
sample0["lotsize"] <- lotsize
```

### I. c) For the regression to make sense we would have to remove the identification number as this doesn't really have anything to do with the properties' valuation. The ID number exists only to make the properties clearly distinguishable from one another. Each number is unique so there is no pattern to observe.

It also appears that the max square footage predictor variable is missing a lot values at a first glance. Using Rstudio to check how many exactly are missing it seems that only 64 values don't have a missing max square footage value. This is less than half the total intended 150 values intended to be used for this report so it would make sense to just not use this variable at all since the missing values will make regression analysis results inaccurate. 

There appeared to be some other missing values not from the max square footage variable when looking at the data set. After counting these there seemed to be 9 other missing values in total which is within the 11 limit we are allowed to remove. Consequently a subset of the data without these points was created which was used for this report and has 141 complete data values.

```{r, echo=F}
# I. c)
set.seed(1003106122)
x <- sample0[complete.cases(sample0$maxsqfoot), ] 
#  This checks how many missing values of the max square footage
# str(x) gives 64 observations only which is less than half the total 150 intended to be used for this report.

sample0$maxsqfoot <- NULL
# this removes the whole max square footage column

# sum(is.na(sample0)) 
# this counts how many values in the subset sample have missing values, I copied the result of this code below and it appears to be 9 which is less than 11 so we can remove them all and use the full and complete data-set 
# [1] 9

sample1 <- na.omit(sample0) 
# this removes all the 'NA' values from the data frame. I thought it would be less confusing to create a new sample1 variable rather than keep changing the sample0 one 
# nrow(sample1) # checking to see if the number of rows is correct, don't need to print this so I make it into a comment
# [1] 141 
# 150 terms minus the 9 NA values counted is euqal to 141 so this should be correct

```

\pagebreak
  
## II. Exploratory Data Analysis
```{r, echo=F}
# II. a) 
# Just addded this to make it easier to find the sub sections.
```
Classifying Variables as Discrete or Continuous

• sale: the actual sale price of the property in Canadian dollars
This variable is discrete.
Note: Although price is measured in numeric dollars, all currencies have a limited fraction of which they can be divided into which makes them a discrete measurement; compared to that of height or mass that can always have infinitesimally smaller increments which is what makes them continuous.

• list: the last list price of the property in Canadian dollars
This variable is discrete.

• bedroom: the total number of bedrooms
This variable is discrete.

• bathroom: the number of bathrooms
This variable is discrete.

• parking: the total number of parking spots
This variable is discrete.

• maxsqfoot: the maximum square footage of the property
This variable is continuous.
Note: The measurements used to create this variable are lengths, which is a physical property that has infinitesimally smaller increments despite any limitations of the smallest increments of the measuring tool used to find said lengths. For that reason this variable is continuous.

• taxes: previous year's property tax
This variable is discrete.

• lotsize: the length in feet of one side of the property multiplied by the frontage in feet
This variable is continuous.

• location: M - Mississauga Neighbourhood, T - Toronto Neighbourhood,
This variable is categorical.


```{r, echo=F}
# II. b)

set.seed(1003106122) 

# making sure to use the same set of randomly drawn from the sample

scatterlabels=c('Actual \n Sale \n Price','Last \n Listing \n Price','Number \n of \n Bedrooms','Number \n of \n Bathrooms','Number \n of \n Parking Spots','Previous \n Year \n Property Tax','Lot \n Size')

# making all the labels for the table because I don't like using the simplified variables

scatter <- pairs(data=sample1,sample1$sale~list+bedroom+bathroom+parking+taxes+lotsize,gap=0.4,cex.labels=0.85,labels=scatterlabels,pch = 20,main = "Matrix of Scatter Plots for Real Estate Data - 6122",upper.panel=NULL,col='pink') 

# creates the scatter plots of all the different variables, thought black was a bit boring so I made it pink. Tried to make the colors of Toronto and Mississauga different but it wouldn't work for some reason.
```

```{r, echo=F,fig.align='center'}
# II. b)

num=cbind('Actual \n Sale \n Price'=sample1$sale,'Last \n Listing \n Price'=sample1$list,'Number \n of \n Bedrooms'=sample1$bedroom,'Number \n of \n Bathrooms'=sample1$bathroom,'Number \n of \n Parking \n Spots'=sample1$parking,'Previous \n Year \n Property \n Tax'=sample1$taxes,'Lot \n Size'=sample1$lotsize)

# prepping the data for the correlation function so it's easier to see in different parts. Also can edit the variable names a bit easier from the table created.

data.cor = round(cor(num),4)

# saving the actual correlation values and rounding the numbers to 4 d.p like in the example from class.

library(corrplot)
# install.packages("corrplot") ; Not sure where to ad these in, but I made a note at the start which packages needed to be installed.Found this when looking for advice on finding pairwise correlations in R. It seemed a lot easier to understand and read than the regular table, hope it's okay to do something a bit different.

plot.cor <- corrplot.mixed(data.cor, lower = 'number', upper = 'shade', number.font = 2,number.digits = 4, tl.col= 'black',tl.cex=0.6,mar=c(0,0,3,0))

# The lower and upper side show the same information given by the 'cor' function just presented a bit differently, the darker the color the more the two variables are correlated. Also removed the unnecessary diagonals that showed correlation of 1 for the same variable. I didn't change the colors because I think the red and blue was quite clear. Chose to do this because it's easier to see the highest and lowest correlation coefficients.

title(main="Pairwise Correlations for Real Estate Data - 6122",cex.main=1)

# just added a title after because when I put it in the corrplot code it wouldn't work for some reason
```

### II. b)
A Note on Interpreting the Matrix: 
The darker the color the higher the correlation. Blue indicates a positive correlation from 0 to 1 and red indicates a negative correlation from 0 to -1. The lower part shows the numeric values while the upper part shows the same information but just as shades of color. The scale is on the right hand side of the matrix.


Correlation Coefficients Ranked from Highest to Lowest

Sale Price / Last Listing Price : 0.9870
Sale Price / Previous Year Property Tax : 0.7596
Sale Price / Number of Bathrooms : 0.6378
Sale Price / Number of Bedrooms : 0.4445
Sale Price / Lot Size : 0.3411
Sale Price / Number of Parking Spots : 0.1363

The highest correlation seem to be between the actual sale price and the listing price. It seems to be significantly higher than all the other correlation coefficients. Property tax and number of bathrooms appear to both be quite high values and close but not as much as last listing price. Number of bedrooms, lot size and number of parking spots seem to be quite low as they are all less than 0.5 (50% correlation). 

### II. c)
For constant variance the distance of the points in the scatter plot, or the distribution around a linear trend, should be somewhat the same. At a glance it seems that quite difficult to tell which single predictor variable this is. After zooming in on the distributions it would seem that lot size does not have constant variance. This can also be checked by plotting the residuals against the fitted values and observing the variance of the points.

```{r, echo=F}
regsize <- lm(sample1$sale~sample1$lotsize)

# running an SLR to start in order to obtain the standardized residuals

sres <- rstandard(regsize)

# Obtaining the standardized residuals from the SLR of lot size

plot(fitted(regsize),sres,main="Standardized Residuals Plotted Against the Fitted Values \n of a Standard Linear Regression Model from Lot Size Predictor Variable - 6122",pch=20,col='pink',cex.main=1,xlab="Fitted Values",ylab = "Standardized Residuals")

# Plotting the standardized residuals against the fitted values for lot size predictor variable

abline(0,0 )

# Adding a line at 0 for reference

```

From the standardized residual plot it seems that the points are not evenly distributed around the zero center line. The values seem much higher above than below the zero line and they are mainly clustered to the left. This confirms that there is violation of the constant variance assumption. 

## III. Methods and Model
```{r, echo=F}
# III. i)
loc<-ifelse(sample1$location== 'T', 1,0)
sample1$location<-loc

# R-created a dummy variable automatically, but I wasn't sure if 0 or 1 was M or T values so I set it myself to T = 1 and M = 0

mreg <- lm(sample1$sale~sample1$list+sample1$bedroom+sample1$bathroom+sample1$parking+sample1$taxes+sample1$lotsize+sample1$location)
summary(mreg)

# Creating the multiple linear regression model.
```

### III. i)

Table of Slope Estimate Values and their Respective P-Values - 6122
```{r, echo=F}
# III. i)

table0 <- cbind(round(summary(mreg)$coefficients[2:8],4),round(summary(mreg)$coefficients[26:32],4))

# creating a table with all the values required in the questions

colnames(table0) <- c("Slope Estimates","P-Values")

# Adding column titles to table

rownames(table0) <- c('Last Listing Price','Number of Bedrooms','Number of Bathrooms','Number of Parking Spots','Previous Year Property Tax','Lot Size','Location')

# Adding row titles to table

table0

# printing the table
```

The benchmark significance level used for this report was 5% (or 0.05). Only three predictor variables had a value less than 0.05 indicating that they are significant; these were last listing price, previous year's property tax and location.

For these significant values, it is indicative that we may draw conclusions from the MLR model created. 

For every 1 dollar increase in the sale price there is a 0.8288 dollar increase expected in the last listing price, assuming all other variables are held constant.

For every 1 dollar increase in the sale price there is a 22.9939 dollar increase expected in the previous year's property tax, assuming all other variables are held constant.

There is a 97460.6846 dollar difference expected between properties in Toronto versus Mississauga 


### III. ii)

```{r, echo=F}
# III. ii)
AIC(mreg)

# checking the AIC value of the original to compare after removing one of the values
```

This is the AIC value for the full model. The number of the bathrooms variable has the highest p-value so this is the first variable to be removed.

A regression model was created for each of the highest respective p-value predictor variables removed as instructed. The summary of the regression was omitted but tables were created with the relevant results and AIC values so it would be easier to read.

```{r, echo=F}
# III. ii)

mreg1 <- lm(sample1$sale~sample1$list+sample1$bedroom+sample1$parking+sample1$taxes+sample1$lotsize+sample1$location)

# summary(mreg1)

# I just removed this and summarized the relevant info in a table because it became difficult to read with the full printed summary, if you want to check it you can un-comment it and run this part again

# number of bathrooms has the highest p-value so this is removed first and the new MLR is created without it

table1 <- cbind(round(summary(mreg1)$coefficients[2:7],4),summary(mreg1)$coefficients[23:28])
colnames(table1) <- c("Slope Estimates","P-Values")
rownames(table1) <- c('Last Listing Price','Number of Bedrooms','Number of Parking Spots','Previous Year Property Tax','Lot Size','Location')
table1
AIC(mreg1)

# checking the AIC of the new model

```

```{r, echo=F}
# III. ii)

# this whole section is just repeated from above until the minimum AIC value is found.

mreg2 <- lm(sample1$sale~sample1$list+sample1$bedroom+sample1$parking+sample1$taxes+sample1$location)
# summary(mreg2)
table2 <- cbind(round(summary(mreg2)$coefficients[2:6],2),summary(mreg2)$coefficients[20:24])
colnames(table2) <- c("Slope Estimates","P-Values")
rownames(table2) <- c('Last Listing Price','Number of Bedrooms','Number of Parking Spots','Previous Year Property Tax','Location')
table2
AIC(mreg2)

mreg3 <- lm(sample1$sale~sample1$list+sample1$parking+sample1$taxes+sample1$location)
# summary(mreg3)
table3 <- cbind(round(summary(mreg3)$coefficients[2:5],4),summary(mreg3)$coefficients[17:20])
colnames(table3) <- c("Slope Estimates","P-Values")
rownames(table3) <- c('Last Listing Price','Number of Parking Spots','Previous Year Property Tax','Location')
table3
AIC(mreg3)

mreg4 <- lm(sample1$sale~sample1$list+sample1$taxes+sample1$location)
# summary(mreg4)
table4 <- cbind(round(summary(mreg4)$coefficients[2:4],4),summary(mreg4)$coefficients[14:16])
colnames(table4) <- c("Slope Estimates","P-Values")
rownames(table4) <- c('Last Listing Price','Previous Year Property Tax','Location')
table4
AIC(mreg4)

mreg5 <- lm(sample1$sale~sample1$list+sample1$taxes)
# summary(mreg5)
table5 <- cbind(round(summary(mreg5)$coefficients[2:3],4),summary(mreg5)$coefficients[11:12])
colnames(table5) <- c("Slope Estimates","P-Values")
rownames(table5) <- c('Last Listing Price','Previous Year Property Tax')
table5
AIC(mreg5)

```

It appears that the minimum predictor values are last listing price, previous year property tax, and location. This is because the AIC value only gets higher once we remove location. 

Therefore the final model would be:

```{r, echo=F}
# III. ii)
summary(mreg4)
```

final sale price = 57600 + 0.8327(last listing price) + 23.39(previous year's property tax) + 130900(location)

This is similar to the original model, however all the predicted coefficients are slightly higher, and the location coefficient is much higher with it enter the 10^5 magnitude versus the 10^4 magnitude.


### III. iii)
The order of the predictor variables from smallest to largest:
listing price, previous year's property taxes, location, parking, number of bedrooms, lot size, number of bathrooms.

```{r, echo=F}
# just doing the reverse of the previous part

mreg6 <- lm(sample1$sale~sample1$list)
# summary(mreg6)
table6 <- cbind(round(summary(mreg6)$coefficients[2],4),summary(mreg6)$coefficients[8])
colnames(table6) <- c("Slope Estimates","P-Values")
rownames(table6) <- c('Last Listing Price')
table6
AIC(mreg6)
```

```{r, echo=F}
mreg7 <- lm(sample1$sale~sample1$list+sample1$taxes)
# summary(mreg7)
table7 <- cbind(round(summary(mreg7)$coefficients[2:3],4),summary(mreg7)$coefficients[11:12])
colnames(table7) <- c("Slope Estimates","P-Values")
rownames(table7) <- c('Last Listing Price','Previous Year Property Tax')
table7
AIC(mreg7)

mreg8 <- lm(sample1$sale~sample1$list+sample1$taxes+sample1$location)
# summary(mreg8)
table8 <- cbind(round(summary(mreg8)$coefficients[2:4],4),summary(mreg8)$coefficients[14:16])
colnames(table8) <- c("Slope Estimates","P-Values")
rownames(table8) <- c('Last Listing Price','Previous Year Property Tax','Location')
table8
AIC(mreg8)

mreg9 <- lm(sample1$sale~sample1$list+sample1$parking+sample1$taxes+sample1$location)
# summary(mreg9)
table9 <- cbind(round(summary(mreg9)$coefficients[2:5],4),summary(mreg9)$coefficients[17:20])
colnames(table9) <- c("Slope Estimates","P-Values")
rownames(table9) <- c('Last Listing Price','Number of Parking Spots','Previous Year Property Tax','Location')
table9
AIC(mreg9)
```

Here the AIC value has risen again as expected and we get the same conclusions from part III. ii) about the minimum number of variables needed for this specific regression; that being, last listing price, previous year's property tax and location. We also see the values differ from part III. i) as expected and in the same way the results in the second part deviate from the first. 

## IV. Discussions and Limitations
### IV. a)
```{r, echo=F}
# IV. a) 6122
par(mfrow=c(2,2))
plot(mreg8,pch=19,col="pink")

# Creating the four diagnostic plots using the 8th regression model, would also be possible to use the fourth one since they're the same

```

### IV. b)

Residual vs Fitted Values

The red line is almost completely horizontal which implies that a linear fit would be appropriate for this data. The majority of values appear to have a constant variance, however there are about three points that the plot function marked out (32, 66, 114) that seemed to deviate from the middle line significantly so they are most likely outliers. The points also seem to cluster to the left then become less frequent towards the right but overall they appear to have similar variances so it would be appropriate to say that the homoscedasticity assumption holds.

Scale-Location

The red line is approximately horizontal with only a dip on the left side of the line. It would be appropriate to say that a linear fit is appropriate for this data. The data seems to be more frequent on the left of the plot then less so on the right, but this is only slightly. The overall spread of the data seems to be randomly distributed around the center line with roughly equal variability. There are three outlying points, 32, 114 and 66 just as found in the residual vs fitted values plot. This is a strong indication that they may be outlying points.

QQ-Plot of Standardized Residuals

It seems the the residuals do follow a normal pattern as they are mostly on the line with only deviation in the tails of a few points. This may indicate they aren't exactly normal, but it appears that the plot function has marked a few of these tail points as outliers; specifically 32, 114 and 66 again. For the most part the points seem to be right on the line so it would be fair to say that the assumption of normally distributed residuals holds.

Residual vs. Leverage

There doesn't appear to be any points that lie outside the calculated cooks distance so that would mean there aren't any leverage points that would significantly affect the results of the data.

### IV. c)
It seems that this reduced model does seem to hold the assumptions required for MLR according to the t-tests from the regression and the plots of the diagnostic plots of the residuals. One could go further and perform extra tests to confirm the conclusions made from diagnostic plots with specific values from the results of these tests. It would also be appropriate to use the final model to create a confidence interval of the predicted values.